{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Practical Tasks. Lesson 3<h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Import modules<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Common code<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_scale(x):\n",
    "    return (x - x.mean()) / x.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    res = 1 / (1 + np.exp(-z))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем датасет и стандартизируем значения фичей\n",
    "X = np.array(\n",
    "            [[   1,    1,  500,    1],\n",
    "            [   1,    1,  700,    1],\n",
    "            [   1,    2,  750,    2],\n",
    "            [   1,    5,  600,    1],\n",
    "            [   1,    3, 1450,    2],\n",
    "            [   1,    0,  800,    1],\n",
    "            [   1,    5, 1500,    3],\n",
    "            [   1,   10, 2000,    3],\n",
    "            [   1,    1,  450,    1],\n",
    "            [   1,    2, 1000,    2]]\n",
    "            )\n",
    "\n",
    "X_st = X.copy().astype(np.float64)\n",
    "for i in range(1, X_st.shape[1]):\n",
    "    X_st[:, i] = standard_scale(X_st[:, i])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([0, 0, 1, 0, 1, 0, 1, 0, 1, 1], dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер массива признаков обучающей выборки (5, 4)\n",
      "Размер массива признаков тестовой выборки (5, 4)\n",
      "Размер массива ответов для обучающей выборки (5,)\n",
      "Размер массива ответов для тестовой выборки (5,)\n"
     ]
    }
   ],
   "source": [
    "# перемешивание\n",
    "np.random.seed(12)\n",
    "shuffle_index = np.random.permutation(X_st.shape[0])\n",
    "X_shuffled, y_shuffled = X[shuffle_index], y[shuffle_index]\n",
    "\n",
    "# разбивка на обучающую и тестовую выборки\n",
    "train_proportion = 0.5\n",
    "train_test_cut = int(len(X) * train_proportion)\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    X_shuffled[:train_test_cut], \\\n",
    "    X_shuffled[train_test_cut:], \\\n",
    "    y_shuffled[:train_test_cut], \\\n",
    "    y_shuffled[train_test_cut:]\n",
    "\n",
    "print(\"Размер массива признаков обучающей выборки\", X_train.shape)\n",
    "print(\"Размер массива признаков тестовой выборки\", X_test.shape)\n",
    "print(\"Размер массива ответов для обучающей выборки\", y_train.shape)\n",
    "print(\"Размер массива ответов для тестовой выборки\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Task 1<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_logloss(y, y_pred):\n",
    "    eps = 1e-16\n",
    "    y_pred_temp = np.clip(y_pred, eps, 1 - eps) # clip zero and one probability values\n",
    "    err = - np.mean(y * np.log(y_pred_temp) + (1.0 - y) * np.log(1.0 - y_pred_temp))\n",
    "    return err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Task 2<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(X, y, iterations, eta=1e-4):\n",
    "    np.random.seed(42)\n",
    "    W = np.random.randn(X.shape[1])\n",
    "    n = X.shape[0]\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        z = np.dot(X, W)\n",
    "        y_pred = sigmoid(z)\n",
    "        # err = calc_logloss(y, y_pred)\n",
    "        \n",
    "        dQ = 1/n * X.T @ (y_pred - y)\n",
    "        W -= eta * dQ\n",
    "        # if i % (iterations / 10) == 0:\n",
    "            # print(i, W, err)\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate arrays of eta's and iteration numbers to find optimal values\n",
    "N = 15\n",
    "N_min, N_max = 100, 10000 # min max number of iterations\n",
    "eta_min, eta_max = 1e-5, 1e-3\n",
    "N_arr = [int(N_min + i*(N_max - N_min) / N) for i in range(N + 1)] # generate N+1 int numbers between N_min, N_max\n",
    "eta_arr = list(np.linspace(eta_min, eta_max, N + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate logloss at different values of parameters and write it into logloss_res array\n",
    "logloss_res = np.zeros((N + 1, N + 1)) # matrix of results\n",
    "for i, eta in enumerate(eta_arr):\n",
    "    for j, N in enumerate(N_arr):\n",
    "        W = eval_model(X_train, y_train, N, eta)\n",
    "        z = np.dot(X_train, W)\n",
    "        y_pred = sigmoid(z)\n",
    "        logloss_res[i, j] = calc_logloss(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min logloss=0.67 at eta=0.00008, iterations=4720\n"
     ]
    }
   ],
   "source": [
    "eta_opt_idx, N_opt_idx = np.unravel_index(logloss_res.argmin(), logloss_res.shape) # find indexes of optimal parameters\n",
    "print(f\"Min logloss={logloss_res[eta_opt_idx, N_opt_idx]:.2f} at eta={eta_arr[eta_opt_idx]:.5f}, iterations={N_arr[N_opt_idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Task 3<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_pred_proba(X, W):\n",
    "    # calculate probabilities\n",
    "    return sigmoid(np.dot(X, W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights vector: [ 0.50060061 -0.23108408 -0.00236107  1.52491793]\n",
      "[0.53411285 0.67523383 0.12372447 0.64882997 0.36207597]\n"
     ]
    }
   ],
   "source": [
    "# find optimal weights and calculate probabilities\n",
    "W_opt = eval_model(X_train, y_train, N_arr[N_opt_idx], eta_arr[eta_opt_idx])\n",
    "print(f\"Weights vector: {W_opt}\")\n",
    "print(calc_pred_proba(X_train, W_opt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Task 4<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_pred(X, W, threshold = 0.5):\n",
    "    # calculate class value\n",
    "    probs = calc_pred_proba(X, W)\n",
    "    return np.where(probs > threshold, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Task 5<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность на обучающей выборке: 40.000\n",
      "Точность на тестовой выборке: 60.000\n"
     ]
    }
   ],
   "source": [
    "def accuracy(y_train, y_test, X_train, X_test, w):\n",
    "    y_predicted_train = calc_pred(X_train, W)\n",
    "    y_predicted_test = calc_pred(X_test, W)\n",
    "    \n",
    "    train_accuracy = np.mean(y_predicted_train == y_train) * 100.0\n",
    "    test_accuracy = np.mean(y_predicted_test == y_test) * 100.0\n",
    "\n",
    "    print(f\"Точность на обучающей выборке: {train_accuracy:.3f}\")\n",
    "    print(f\"Точность на тестовой выборке: {test_accuracy:.3f}\")\n",
    "\n",
    "accuracy(y_train, y_test, X_train, X_test, W_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод: нет признака переобучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1]\n",
      " [2 1]]\n",
      "[[3 0]\n",
      " [1 1]]\n"
     ]
    }
   ],
   "source": [
    "def error_matrix(X, y, W):\n",
    "    # naive realization\n",
    "    y_predicted = calc_pred(X, W)\n",
    "    TP, FP, FN, TN = 0, 0, 0, 0\n",
    "    for y, y_pred in zip(y, y_predicted):\n",
    "        if y == y_pred:\n",
    "            if y: TP += 1\n",
    "            else: TN += 1\n",
    "        else:\n",
    "            if y: FP += 1\n",
    "            else: FN += 1\n",
    "    return np.array([[TP, FP], [FN, TN]])\n",
    "\n",
    "error_matrix_train = error_matrix(X_train, y_train, W_opt)\n",
    "error_matrix_test = error_matrix(X_test, y_test, W_opt)\n",
    "print(error_matrix_train)\n",
    "print(error_matrix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_train=0.50, recall_train=0.33, F1_train=0.40\n",
      "precision_test=1.00, recall_test=0.75, F1_test=0.86\n"
     ]
    }
   ],
   "source": [
    "def metrics_calculation(error_matrix):\n",
    "    TP, FP, FN, TN = error_matrix.flatten()\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    F1 = 2 * recall * precision / (recall + precision)\n",
    "    return precision, recall, F1\n",
    "\n",
    "precision_train, recall_train, F1_train = metrics_calculation(error_matrix_train)\n",
    "precision_test, recall_test, F1_test = metrics_calculation(error_matrix_test)\n",
    "print(f\"precision_train={precision_train:.2f}, recall_train={recall_train:.2f}, F1_train={F1_train:.2f}\")\n",
    "print(f\"precision_test={precision_test:.2f}, recall_test={recall_test:.2f}, F1_test={F1_test:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Task 6<h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод: на тестовой выборке показатели качества выше, признаков переобучения нет. Вывод является предварительным. Ввиду малости выборки объекты разных класов могли неравномерно распределиться между обучающим и тестовым датасетом. Вследствие этого возможно переобучение при применении алгоритма на более крупных выборках"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "251e81559002ddd7b2bf032dcaac0eab8c07dc3f187a921f1aa359dc2a69befe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "251e81559002ddd7b2bf032dcaac0eab8c07dc3f187a921f1aa359dc2a69befe"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
